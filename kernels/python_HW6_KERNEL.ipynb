{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4d57096d05c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0ml2distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvisclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cvxpy'"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "from cvxpy import *\n",
    "import l2distance\n",
    "import visclassifier\n",
    "#</GRADED>\n",
    "import matplotlib\n",
    "#matplotlib.use('PDF')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"yinyang.png\" width=\"400px\" /></a>\n",
    "    </center>\n",
    "      <p><cite><center>\"Just as we have two eyes and two feet,<br>\n",
    "      duality is part of life.\"<br>\n",
    "<b>--Carlos Santana</b><br>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a linear support vector machine and one operating in kernel space. For this you will need to formulate the primal and dual optimization problems as quadratic programs. You will be using <code>cvxpy</code>. Before we get started please read through the <a href=\"http://www.cvxpy.org/en/latest/tutorial/index.html\">tutorial</a> of cvxpy and its quadratic programming solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Linear classification</h4>\n",
    "\n",
    "<p> The first assignment is to implement a linear support vector machine. Before we get started we can generate some data to see if everything is working:  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrandomdata(n=100,b=0.):\n",
    "    # generate random data and linearly separagle labels\n",
    "    xTr = np.random.randn(n, 2)\n",
    "    # defining random hyperplane\n",
    "    w0 = np.random.rand(2, 1)\n",
    "    # assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "    yTr = np.sign(np.dot(xTr, w0)+b).flatten()\n",
    "    return xTr, yTr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remember the SVM primal formulation\n",
    "$$\\begin{aligned}\n",
    "             &\\min_{\\mathbf{w},b,\\xi} \\|\\mathbf{w}\\|^2_2+C \\sum_{i=1}^n \\xi_i\\\\\n",
    "       & \\text{such that }  \\ \\forall i:\\\\\n",
    "             & y_i(\\mathbf{w}^\\top \\mathbf{x}_i+b)\\geq 1-\\xi_i\\\\\n",
    "             & \\xi_i\\geq 0.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "You will need to implement  the function <code>primalSVM</code>, which takes in training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$. Currently, the code below is a placeholder example of a <code>cvxpy</code> optimization problem. You need to update the objective, the constraints and introduce new variables to output the correct hyperplane and bias. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def primalSVM(xTr, yTr, C=1):\n",
    "    \"\"\"\n",
    "    function (classifier,w,b) = primalSVM(xTr,yTr;C=1)\n",
    "    constructs the SVM primal formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (n)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        fun   | usage: predictions=fun(xTe); predictions.shape = (n,)\n",
    "        wout  | the weight vector calculated by the solver\n",
    "        bout  | the bias term calculated by the solver\n",
    "    \"\"\"\n",
    "    N, d = xTr.shape\n",
    "    y = yTr.flatten()\n",
    "    ## Solution Start\n",
    "    w = Variable(d)\n",
    "    b = Variable(1)\n",
    "    eps = Variable(N)\n",
    "    objective = Minimize(sum_squares(w) + C * sum(eps))\n",
    "#     constraints = [eps >= 0, yTr * (xTr * w + b) >= 1 - eps]\n",
    "    constraints = [eps >= 0, multiply(yTr, (xTr * w + b)) >= 1 - eps]\n",
    "    prob = Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    wout = w.value\n",
    "    bout = b.value\n",
    "    \n",
    "    ## Solution End\n",
    "    \n",
    "    fun = lambda x: x.dot(wout) + bout\n",
    "    return fun, wout, bout\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test your SVM primal solver with the following randomly generated data set. We label it in a way that it is guaranteed to be linearly separable. If your code works correctly the hyper-plane should separate all the $x$'s into the red half and all the $o$'s into the blue half. With sufficiently large values of $C$ (e.g. $C>10$) you should obtain $0\\%$ training error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayify(x):\n",
    "    \"\"\"flattens and converts to numpy\"\"\"\n",
    "    return np.array(x).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1ee0b9b3f2c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mxTr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myTr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenrandomdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprimalSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myTr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvisclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxTr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myTr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0828dbfd5264>\u001b[0m in \u001b[0;36mprimalSVM\u001b[1;34m(xTr, yTr, C)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myTr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# dummy code: example of establishing objective and constraints, and let the solver solve it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "xTr,yTr=genrandomdata()\n",
    "fun,w,b=primalSVM(xTr,yTr,C=10)\n",
    "visclassifier.visclassifier(fun,xTr,yTr,w=w,b=b)\n",
    "\n",
    "\n",
    "err=np.mean(arrayify(np.sign(fun(xTr)))!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions used to create animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateboundary():\n",
    "    global w,b,Xdata,ldata,stepsize\n",
    "\n",
    "    _, w_pre, b_pre = primalSVM(np.transpose(Xdata),np.array(ldata),C=10)\n",
    "    w = np.array(w_pre).reshape(-1)\n",
    "    b = b_pre\n",
    "    stepsize+=1\n",
    "\n",
    "def updatescreen():\n",
    "    global w,b,ax,line \n",
    "    q=-b/(w**2).sum()*w;\n",
    "    if line==None:\n",
    "        line, = ax.plot([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]],'b--')\n",
    "    else:\n",
    "        line.set_ydata([q[1]+w[0],q[1]-w[0]])\n",
    "        line.set_xdata([q[0]-w[1],q[0]+w[1]])\n",
    "    \n",
    "def animate(i):\n",
    "    if len(ldata)>0 and (min(ldata)+max(ldata)==0):\n",
    "        if stepsize<1000:\n",
    "            updateboundary()\n",
    "            updatescreen();\n",
    "    \n",
    "def onclick(event):\n",
    "    global Xdata, stepsize  \n",
    "    if event.key == 'shift': # add positive point\n",
    "        ax.plot(event.xdata,event.ydata,'or')\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        ax.plot(event.xdata,event.ydata,'ob')\n",
    "        label=-1    \n",
    "    pos=np.array([[event.xdata],[event.ydata]])\n",
    "    ldata.append(label);\n",
    "    Xdata=np.hstack((Xdata,pos))\n",
    "    stepsize=1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata=pylab.rand(2,0)\n",
    "ldata=[]\n",
    "w=[]\n",
    "b=[]\n",
    "line=None\n",
    "stepsize=1;\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "ani = FuncAnimation(fig, animate,pylab.arange(1,100,1),interval=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Spiral data set</h4>\n",
    "\n",
    "<p>The linear classifier works great in simple linear cases. But what if the data is more complicated? We provide you with a \"spiral\" data set. You can load it and visualize it with the following two code snippets:\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr,yTr,xTe,yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "plt.scatter(xTr[yTr == 1, 0], xTr[yTr == 1, 1], c='b')\n",
    "plt.scatter(xTr[yTr != 1, 0], xTr[yTr != 1, 1], c='r')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you apply your previously functioning linear classifier on this data set you will see that you get terrible results. Your training error will increase drastically. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fun,w,b=primalSVM(xTr,yTr,C=10)\n",
    "visclassifier(fun,xTr,yTr,w=[],b=0)\n",
    "err=np.mean(arrayify(np.sign(fun(xTr)))!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing a kernelized SVM</h3>\n",
    "\n",
    "<p> For a data set as complex as the spiral data set, you will need a more complex classifier. \n",
    "First implement the kernel function\n",
    "<pre>\tcomputeK(kerneltype,X,Z,kpar)</pre>\n",
    "It takes as input a kernel type (kerneltype) and two data sets $\\mathbf{X}$ in $\\mathcal{R}^{n\\times d}$ and $\\mathbf{Z}$ in $\\mathcal{R}^{m\\times d}$ and outputs a kernel matrix $\\mathbf{K}\\in{\\mathcal{R}^{n\\times m}}$. The last input, <code>kpar</code> specifies the kernel parameter (e.g. the inverse kernel width $\\gamma$ in the RBF case or the degree $p$ in the polynomial case.)\n",
    "\t<ol>\n",
    "\t<li>For the linear kernel (<code>ktype='linear'</code>) svm, use $k(\\mathbf{x},\\mathbf{z})=x^Tz$ </li> \n",
    "\t<li>For the radial basis function kernel (<code>ktype='rbf'</code>) svm use $k(\\mathbf{x},\\mathbf{z})=\\exp(-\\gamma ||x-z||^2)$ (gamma is a hyperparameter, passed a the value of kpar)</li>\n",
    "\t<li>For the polynomial kernel (<code>ktype='poly'</code>) use  $k(\\mathbf{x},\\mathbf{z})=(x^Tz + 1)^d$ (d is the degree of the polymial, passed as the value of kpar)</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can use the function <b><code>l2distance</code></b> as a helperfunction.</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def computeK(kerneltype, X, Z, kpar=0):\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=k(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (nxd);\n",
    "    Z: m input vectors of dimension d (mxd);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel matrix\n",
    "    \"\"\"\n",
    "    assert kerneltype in [\"linear\",\"polynomial\",\"poly\",\"rbf\"], \"Kernel type %s not known.\" % kerneltype\n",
    "    assert X.shape[1] == Z.shape[1], \"Input dimensions do not match\"\n",
    "    \n",
    "    ## Solution Start\n",
    "    K = None\n",
    "    if kerneltype == 'linear':\n",
    "        K = X.dot(Z.T)\n",
    "    elif kerneltype == 'polynomial' or kerneltype == 'poly':\n",
    "        K = (1 + X.dot(Z.T))**kpar\n",
    "    elif kerneltype == 'rbf':\n",
    "        D = (l2distance.l2distance(X, Z))**2\n",
    "        assert D.shape[0] == X.shape[0]\n",
    "        assert D.shape[1] == Z.shape[0]\n",
    "        expo = -D * (kpar)\n",
    "#         print(expo[0][:20])\n",
    "        K = np.exp(expo)\n",
    "#         print(K[0][:20])\n",
    "    ## Solution End\n",
    "    \n",
    "    return K\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following code snippet plots an image of the kernel matrix for the data points in the spiral set. Use it to test your <b><code>computeK</code></b> function:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "K=computeK(\"rbf\",xTr,xTr,kpar=0.05)\n",
    "# plot an image of the kernel matrix\n",
    "plt.pcolormesh(K, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the SVM optimization has the following dual formulation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "             &\\min_{\\alpha_1,\\cdots,\\alpha_n}\\frac{1}{2} \\sum_{i,j}\\alpha_i \\alpha_j y_i y_j \\mathbf{K}_{ij} - \\sum_{i=1}^{n}\\alpha_i  \\\\\n",
    "       \\text{s.t.}  &\\quad 0 \\leq \\alpha_i \\leq C\\\\\n",
    "             &\\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "This is equivalent to solving for the SVM primal\n",
    "$$ L(\\mathbf{w},b) = C\\sum_{i=1}^n \\max(1-y_i(\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)+b),0) + ||w||_2^2$$\n",
    "where $\\mathbf{w}=\\sum_{i=1}^n y_i \\alpha_i \\phi(\\mathbf{x}_i)$ and $\\mathbf{K}_{ij}=k(\\mathbf{x}_i,\\mathbf{x}_j)=\\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)$, for some mapping $\\phi(\\cdot)$.  Please note that here all $\\alpha_i\\geq 0$, which is possible because we multiply by $y_i$ in the definition of $\\mathbf{w}$. One advantage of keeping all $\\alpha_i$ non-negative is that we can easily identify non-support vectors as vectors with $\\alpha_i=0$. \n",
    "\n",
    "<p>Implement the function <code>dualqp</code>, which takes as input a kernel matrix $K$, a vector of labels $yTr$ in $\\mathcal{R}^{n}$, and a regularization constant $C\\geq 0$. This function should solve the quadratic optimization problem and output the optimal vector $\\mathbf{\\alpha}\\in{\\mathcal{R}^n}$.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def dualqp(K,yTr,C):\n",
    "    \"\"\"\n",
    "    function alpha = dualqp(K,yTr,C)\n",
    "    constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        K     | the (nxn) kernel matrix\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        alpha | the calculated solution vector (nx1)\n",
    "    \"\"\"\n",
    "    y = yTr.flatten()\n",
    "    N, _ = K.shape\n",
    "    alpha = Variable(N)\n",
    "    \n",
    "    ## Solution Start\n",
    "#     print(K)\n",
    "#     print(all(np.linalg.eigvals(K)>=0))\n",
    "#     objective = Minimize(0.5 * alpha.T  * K * alpha - sum(alpha))\n",
    "    objective = Minimize(0.5 * quad_form(multiply(alpha, yTr), K) - sum(alpha))\n",
    "    constraints = [\n",
    "        alpha >= 0,\n",
    "        alpha <= C,\n",
    "        yTr.T * alpha == 0\n",
    "    ]\n",
    "    prob = Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "#     print(prob.status)\n",
    "#     print(alpha.value)\n",
    "    \n",
    "    ## Solution End\n",
    "    \n",
    "    return np.array(alpha.value).flatten()\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows a usecase of how <code>dualqp</code> could be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = 10\n",
    "lmbda = 0.25\n",
    "ktype = \"rbf\"\n",
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "# compute kernel (make sure it is PSD)\n",
    "K = computeK(ktype,xTr,xTr)\n",
    "eps = 1e-10\n",
    "# make sure it is symmetric and positive semi-definite\n",
    "K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "\n",
    "alpha=dualqp(K,yTr,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now that you can solve the dual correctly, you should have the values for $\\alpha_i$. But you are not done yet. You still need to be able to classify new test points. Remember from class that $h(\\mathbf{x})=\\sum_{i=1}^n \\alpha_i y_i k(\\mathbf{x}_i,\\mathbf{x})+b$. You need to obtain $b$. It is easy to show (and omitted here) that if $C>\\alpha_i>0$ (with strict $>$), then we must have that $y_i(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i)+b)=1$. Rephrase this equality in terms of $\\alpha_i$ and solve for $b$. Implement\n",
    "\n",
    "<p> b=recoverBias(K,yTr,alphas,C); </p>\n",
    "\n",
    "<p> where <code>b</code> is the hyperplane bias.\n",
    "(Hint: This is most stable if you pick an $\\alpha_i$ that is furthest from $C$ and $0$. )</p>\n",
    "\n",
    "<p>Please note that this use of the word bias has absolutely nothing to do with the word bias in the bias variance trade-off. It is just the same word but two completely different meanings. This unfortunate term collision comes from the fact that we are borrowing concepts from geometry and statistics.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def recoverBias(K,yTr,alpha,C):\n",
    "    \"\"\"\n",
    "    function bias=recoverBias(K,yTr,alpha,C);\n",
    "    Solves for the hyperplane bias term, which is uniquely specified by the \n",
    "    support vectors with alpha values 0<alpha<C\n",
    "    \n",
    "    INPUT:\n",
    "    K : nxn kernel matrix\n",
    "    yTr : nx1 input labels\n",
    "    alpha  : nx1 vector of alpha values\n",
    "    C : regularization constant\n",
    "    \n",
    "    Output:\n",
    "    bias : the scalar hyperplane bias of the kernel SVM specified by alphas\n",
    "    \"\"\"\n",
    "\n",
    "    ## Solution Start\n",
    "\n",
    "    idx = np.argmin(np.abs(alpha - C*0.5))\n",
    "   \n",
    "    bias = yTr[idx] - np.multiply(yTr , alpha).dot(K[:,idx])\n",
    "    ## Solution End\n",
    "    \n",
    "    return bias\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Test your <b><code>recoverBias</code></b> function with the following code, which uses the dual solver on a linearly separable dataset:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTr,yTr=genrandomdata(b=0.5)\n",
    "C=10\n",
    "K=computeK(\"linear\",xTr,xTr)\n",
    "eps=1e-10\n",
    "K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "alpha = dualqp(K,yTr,C)\n",
    "ba=recoverBias(K,yTr,alpha,C)\n",
    "wa = (alpha * yTr).dot(xTr)\n",
    "fun = lambda x: x.dot(wa) + ba\n",
    "visclassifier(fun, xTr, yTr, w=wa, b=ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    svmclassify=dualSVM(xTr,yTr,C,ktype,kpar);\n",
    "    </pre>\n",
    "    It should use your functions <code><b>computeK</b></code> and <code><b>generateQP</b></code> to solve the SVM dual problem of an SVM specified by a training data set (<code><b>xTr,yTr</b></code>), a regularization parameter (<code>C</code>), a kernel type (<code>ktype</code>) and kernel parameter (<code>lmbda</code>, to be used as kpar in Kernel construction). Then, find the support vectors and recover the bias to return <b><code>svmclassify</code></b>, a function that uses your SVM to classify a set of test points <code>xTe</code>.\n",
    "\n",
    "<b>Hint: You need to ensure that the kernel matrix is positive semi-definite during training. The best way to do this is to make sure it is strictly symmetric and to add the identity matrix to it, multiplied by a tiny epsilon value.</b>\n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def testkernel(X,Z,kpar):\n",
    "    D = (l2distance.l2distance(X, Z))**2\n",
    "    expo = -D * (kpar)\n",
    "#         print(expo[0][:20])\n",
    "    K = np.exp(expo)\n",
    "def dualSVM(xTr,yTr,C,ktype,lmbda,eps=1e-10):\n",
    "    \"\"\"\n",
    "    function classifier = dualSVM(xTr,yTr,C,ktype,lmbda);\n",
    "    Constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "        ktype | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        lmbda | the kernel parameter - degree for poly, inverse width for rbf\n",
    "    \n",
    "    Output:\n",
    "        svmclassify | usage: predictions=svmclassify(xTe);\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Solution Start\n",
    "    K = computeK(ktype, xTr, xTr,lmbda)\n",
    "\n",
    "    K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "    alpha = dualqp(K,yTr,C)\n",
    "    ba=recoverBias(K,yTr,alpha,C)\n",
    "  \n",
    "\n",
    "  #  svmclassify = lambda x: computeK(ktype, xTr, xTr[0],lmbda).dot((alpha * yTr))+ba #Dummy code\n",
    "\n",
    "    svmclassify = lambda x: np.dot(np.multiply(alpha,yTr),computeK(ktype, xTr, x, lmbda))+ba\n",
    "    # Solution End\n",
    "    \n",
    "   \n",
    "    \n",
    "    return svmclassify\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we try the SVM with RBF kernel on the spiral data. If you implemented it correctly, train and test error should be close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "C=10.0\n",
    "sigma=0.25\n",
    "ktype=\"rbf\"\n",
    "svmclassify=dualSVM(xTr,yTr,C,ktype,sigma)\n",
    "\n",
    "visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# compute training and testing error\n",
    "predsTr=svmclassify(xTr)\n",
    "trainingerr=np.mean(np.sign(predsTr)!=yTr)\n",
    "print(\"Training error: %2.4f\" % trainingerr)\n",
    "\n",
    "predsTe=svmclassify(xTe)\n",
    "testingerr=np.mean(np.sign(predsTe)!=yTe)\n",
    "print(\"Testing error: %2.4f\" % testingerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are pretty sensitive to hyper-parameters. We can visualize the results of a hyper-parameter grid search as a heat-map, where we sweep across different values of C and kpar and output the result on a validation dataset. Now we ask you to implement a cross-validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList):\n",
    "    \"\"\"\n",
    "    function bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList);\n",
    "    Use the parameter search to find the optimal parameter,\n",
    "    Individual models are trained on (xTr,yTr) while validated on (xValid,yValid)\n",
    "    \n",
    "    Input:\n",
    "        xTr      | training data (nxd)\n",
    "        yTr      | training labels (nx1)\n",
    "        xValid   | training data (mxd)\n",
    "        yValid   | training labels (mx1)\n",
    "        ktype    | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        CList    | The list of values to try for the SVM regularization parameter C (ax1)\n",
    "        lmbdaList| The list of values to try for the kernel parameter lmbda- degree for poly, inverse width for rbf (bx1)\n",
    "    \n",
    "    Output:\n",
    "        bestC      | the best C parameter\n",
    "        bestLmbda  | the best Lmbda parameter\n",
    "        ErrorMatrix| the test error rate for each given C and Lmbda when trained on (xTr,yTr) and tested on (xValid,yValid),(axb)\n",
    "    \"\"\"\n",
    "    # gridsearch for best parameters\n",
    "    ErrorMatrix=np.zeros((len(CList),len(lmbdaList)))\n",
    "    bestC,bestLmbda = 0.,0.\n",
    "    \n",
    "    # Start Solution\n",
    "    min_error=float('inf')\n",
    "    for i in range(len(CList)):\n",
    "        C = CList[i]\n",
    "        for j in range(len(lmbdaList)):\n",
    "            lmbda=lmbdaList[j]\n",
    "            svmclassify = dualSVM(xTr,yTr,C,ktype,lmbda)\n",
    "            pre = svmclassify(xValid)\n",
    "            error = np.mean(np.sign(pre) != yValid)\n",
    "            ErrorMatrix[i, j] = error\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                bestC = C\n",
    "                bestlmbda = lmbda\n",
    "    # End Solution\n",
    "            \n",
    "    return bestC,bestLmbda,ErrorMatrix\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xValid,yValid=spiraldata(100)\n",
    "CList=(2.0**np.linspace(-1,5,7))\n",
    "lmbdaList=(np.linspace(0.1,0.5,5))\n",
    "\n",
    "bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,'rbf',CList,lmbdaList)\n",
    "\n",
    "plt.pcolormesh(ErrorMatrix, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"lmbda_idx\")\n",
    "plt.ylabel(\"C_idx\")\n",
    "plt.title(\"Validation error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented everything correctly, the result should look similar to this image:\n",
    "<center>\n",
    " <img src=\"crossval.png\" width=\"300px\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition: we ask you to implement function autosvm, which given xTr and yTr, splits them into training data and validation data, and then uses a hyperparameter search to find the optimal hyper parameters. \n",
    "\n",
    "Function autosvm should return a function which will act as a classifier on xTe.\n",
    "\n",
    "You have a 5 minute time limit on multiple datasets, each dataset having different optimal hyperparameters, so you should strive for a good method of finding hyperparameters (within the time limit) instead of just trying to find a static set of good hyperparameters. \n",
    "\n",
    "You will get extra credit for the competition if you can beat the base benchmark of 34% error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def autosvm(xTr,yTr):\n",
    "    \"\"\"\n",
    "    svmclassify = autosvm(xTr,yTr), where yTe = svmclassify(xTe)\n",
    "    \n",
    "     cross_validation(xTr,yTr,xValid,yValid,'rbf',CList,lmbdaList)\n",
    "    \n",
    "    \"\"\"\n",
    "    length=xTr.shape[0]\n",
    "    last_idx=0\n",
    "    folds=5 #k-fold cross validation\n",
    "    ktype=\"rbf\"\n",
    "    p=np.random.permutation(len(xTr))\n",
    "    t=0\n",
    "    min_error=float('inf')\n",
    "   \n",
    "    while(t<30):\n",
    "        C=np.random.uniform(1,20)\n",
    "        lmbda=np.random.uniform(0, 1)  \n",
    "        err=np.zeros((folds),dtype=float)\n",
    "\n",
    "        for i in range (folds):\n",
    "            n1=last_idx    \n",
    "            n2=int(length/folds)\n",
    "            tst_idx=p[n1:n1+n2]\n",
    "            trn_idx=np.delete(p, tst_idx)\n",
    "\n",
    "            last_idx=n1+n2\n",
    "\n",
    "            xtrn = xTr[trn_idx]\n",
    "            ytrn = yTr[trn_idx]\n",
    "            xval = xTr[tst_idx]\n",
    "            yval = yTr[tst_idx]\n",
    "            \n",
    "            svmclassify = dualSVM(xtrn,ytrn,C,ktype,lmbda)\n",
    "            pre = svmclassify(xval)\n",
    "            error = np.mean(np.sign(pre) != yval)\n",
    "            err[i]=error\n",
    "        avg_error=np.mean(err)    \n",
    "        if avg_error < min_error:\n",
    "            min_error=avg_error\n",
    "            bestC = C\n",
    "            bestlmbda=lmbda\n",
    "        t+=1\n",
    "           \n",
    "  \n",
    "\n",
    "    return dualSVM(xTr,yTr,bestC,ktype,bestlmbda)\n",
    "\n",
    "\n",
    "    # (Optional) TODO 7\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=xTr.shape[0]\n",
    "p=np.random.permutation(len(xTr))\n",
    "m=0.8*length\n",
    "n=length\n",
    "trn_idx=p[:m]\n",
    "tst_idx=p[m:]\n",
    "xtrn = xTr[trn_idx]\n",
    "ytrn = yTr[trn_idx]\n",
    "xval = xTr[tst_idx]\n",
    "yval = yTr[tst_idx]\n",
    "x=np.zeros(m,2)\n",
    "y=np.zeros(m,1)\n",
    "ktype=\"rbf\"\n",
    "def computeerror(xtrn,ytrn,xval,yval,x):\n",
    "     svmclassify = dualSVM(xtrn,ytrn,x[0],\"rbf\",x[1])\n",
    "     pre = svmclassify(xval)\n",
    "     error = np.mean(np.sign(pre) != yval)\n",
    "     return error\n",
    "for i in range (0,m):\n",
    "    x[i,0]=np.random.uniform(0,100) #c\n",
    "    x[i,1]=np.random.uniform(0, 1)  #lmbda\n",
    "    y[i]=computeerror(xtrn,ytrn,xval,yval,x[i])\n",
    "K=computeK(ktype, x[:m],x[:m])\n",
    "for i in range(m,n):\n",
    "    \n",
    "    K1=computeK(ktype, x[:m], x[m:i+1])\n",
    "    K2=computeK(ktype,x[m:i+1],x[m:i+1])\n",
    "    d=arg(K1.T.dot(np.linalg.inv(K))).dot(y)-0.1*np.sqrt(K2-(K1.T.dot(np.linalg.inv(K))).dot(K1))\n",
    "    \n",
    "    y[i]=computeerror(xtrn,ytrn,xval,yval,x[i])\n",
    "    x[i-1]=np.min(x[:i-1])-\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
